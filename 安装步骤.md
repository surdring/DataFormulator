# Data Formulator 安装指南

本文档提供在 Linux 系统上安装 Data Formulator 的详细步骤。

## 系统要求

- Python ≥ 3.9（推荐使用 3.11+）
- Node.js
- Yarn

## 安装步骤

### 1. 克隆仓库（如尚未克隆）

```bash
git clone https://github.com/microsoft/data-formulator.git
cd data-formulator
```

### 2. 创建并激活 Python 虚拟环境

```bash
# 创建虚拟环境
python3 -m venv venv

# 激活虚拟环境
source venv/bin/activate

# 可选：升级 pip
python -m pip install --upgrade pip
```

# 配置阿里云镜像源（可选，国内用户推荐）
```bash
# 临时使用阿里云源
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com

# 或永久配置（推荐）
mkdir -p ~/.pip
echo -e "[global]\nindex-url = https://mirrors.aliyun.com/pypi/simple/\ntrusted-host = mirrors.aliyun.com" > ~/.pip/pip.conf
```

### 3. 安装 Python 依赖

```bash
# 安装所有依赖（包括 Data Formulator 本身）
pip install -r requirements.txt
```

### 4. 安装 Node.js 依赖

```bash
# 安装 Yarn（如尚未安装）
npm install -g yarn

# 安装前端依赖
yarn
```

### 5. 配置环境变量（可选）

1. 复制示例环境变量文件：
   ```bash
   cp .env.template .env
   cp api-keys.env.template api-keys.env
   ```

2. 按需编辑 `.env` 和 `api-keys.env` 文件配置您的设置和 API 密钥

## 使用本地部署的 LLM（llama.cpp）

Data Formulator 不直接启动或管理 `llama.cpp`，只要你的本地模型通过 HTTP 暴露为 **OpenAI 兼容接口**（或通过 Ollama 提供接口），就可以在应用中使用。

### 1. 直接使用本地 llama.cpp server（推荐）

1. 使用类似下面的命令启动 `llama-server`（根据你实际路径调整）：
   ```bash
   /home/zhengxueen/workspace/llama.cpp/build-hip/bin/llama-server \
     -m /mnt/ssd/models/gpt-oss-20b-mxfp4.gguf \
     -c 0 --n-gpu-layers -1 --jinja \
     --host 0.0.0.0 --port 8080 --alias gpt-oss-20b
   ```
   > 可选：可以在命令中增加 `--api-key your-local-key` 开启简单鉴权；如果开启，需要与下面 `OPENAI_API_KEY` 保持一致。

2. 如果还没有 `api-keys.env`，先复制模板：
   ```bash
   cp api-keys.env.template api-keys.env
   ```

3. 编辑 `api-keys.env`，启用本地 OpenAI 兼容接口（注意端口和模型名）：
   ```env
   OPENAI_ENABLED=true
   OPENAI_API_BASE=http://localhost:8080/v1
   OPENAI_API_KEY=dummy-key          # 如果 llama-server 未设置 --api-key，可以填任意占位值
   OPENAI_MODELS=gpt-oss-20b   # 与你在请求中使用的模型名称保持一致
   ```

4. 重启 Data Formulator 后，在应用右上角模型配置对话框中：
   - 选择 provider 为 `openai`
   - 选择 `gpt-oss-20b`
   - 点击 “Test” 按钮，确认模型状态为 `Ready`

### 2. 通过 Ollama 使用本地模型（可选）

如果你更习惯使用 Ollama 管理本地模型，也可以通过 Ollama 暴露接口给 Data Formulator：

1. 安装 Ollama（参考官方文档：https://ollama.com/download）。
2. 启动 Ollama，并下载你需要的模型，例如：
   ```bash
   ollama pull llama3
   ```
3. 确保 Ollama 服务在本机运行，默认地址为：
   ```bash
   http://localhost:11434
   ```

4. 编辑 `api-keys.env`，启用 Ollama 本地模型：
   ```bash
   OLLAMA_ENABLED=true
   OLLAMA_API_BASE=http://localhost:11434
   OLLAMA_MODELS=llama3:latest   # 或你在 Ollama 中实际下载的模型名，如 llama3:8b
   ```

5. 重新启动 Data Formulator 后，在应用右上角的模型配置对话框中：
   - 选择 provider 为 `ollama`
   - 选择你在 `OLLAMA_MODELS` 中配置的模型
   - 点击 “Test” 按钮，确认模型状态为 `Ready`

## 运行应用

### 开发模式

```bash
# 确保已激活虚拟环境
cd /home/zhengxueen/workspace/data-formulator
source venv/bin/activate

# 如果是第一次运行，或前端代码有修改，需要先构建前端静态资源
yarn build

# 启动开发服务器（方式一：已赋予执行权限时）
./local_server.sh
```

如果执行 `./local_server.sh` 提示 `权限不够`，可以先给脚本加执行权限，或直接用 bash 运行：

```bash
# 方式二：先赋予执行权限再运行（推荐）
chmod +x local_server.sh
./local_server.sh

# 方式三：不改权限，直接用 bash 运行
bash local_server.sh
```

然后在浏览器中访问：http://localhost:5000

### 生产模式

1. 构建前端：
   ```bash
   yarn build
   ```

2. 构建 Python 包：
   ```bash
   pip install build
   python -m build
   ```

3. 安装构建好的包：
   ```bash
   pip install dist/data_formulator-*.whl
   ```

4. 运行：
   ```bash
   data_formulator
   ```

## 验证安装

```bash
# 检查 Python 包是否安装成功
python -c "import data_formulator; print(data_formulator.__version__)"

# 检查命令行工具是否可用
data_formulator --help
```

## 更新

```bash
# 拉取最新代码
git pull

# 重新安装依赖
pip install -r requirements.txt

# 更新前端依赖
yarn
```

## 问题排查

- 如果遇到 `pyodbc` 安装问题，请确保已安装系统依赖：
  ```bash
  # Ubuntu/Debian
  sudo apt-get install unixodbc-dev
  
  # CentOS/RHEL
  sudo yum install unixODBC-devel
  ```

- 如果遇到其他依赖问题，请检查 Python 版本是否符合要求（≥3.9）

## 安全说明

- 生产环境部署时，请确保配置适当的安全措施
- 不要将 API 密钥提交到版本控制系统
- 考虑使用 `--disable-database` 参数以禁用本地数据库功能
