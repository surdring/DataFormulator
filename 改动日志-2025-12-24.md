# Data Formulator 改动日志（2025-12-24）

## 目的
- **恢复中文界面（i18n）**：从升级到 v0.5.1 后被覆盖/丢失的中文化改动自动恢复。
- **修复本地 llama.cpp（OpenAI 兼容接口）联通与流式输出**：解决模型测试 502、SSE 流式被截断、以及流式兼容问题。

## 仓库状态快照
- **HEAD**：`38d75a3`
- **今日/本阶段提交记录**（按时间顺序）：
  - `62a1206` chore: reapply localization changes on 0.5.1
  - `b24c81e` chore: update yarn.lock after upgrade
  - `4dbe164` chore: before restore i18n from backup
  - `39c93e0` restore_i18n_from_backup.py
  - `ed0de66` 第二次提交更新restore_i18n_from_backup.py
  - `38d75a3` 第三次提交更新restore_i18n_from_backup.py

- **当前工作区未提交改动文件**（`git status --porcelain`）：
  - `py-src/data_formulator/agent_routes.py`
  - `py-src/data_formulator/agents/client_utils.py`
  - `py-src/data_formulator/agents/agent_report_gen.py`
  - `src/app/App.tsx`
  - `src/views/About.tsx`
  - `src/views/AgentRulesDialog.tsx`
  - `src/views/ChartRecBox.tsx`
  - `src/views/ConceptShelf.tsx`
  - `src/views/DBTableManager.tsx`
  - `src/views/DataLoadingChat.tsx`
  - `src/views/DerivedDataDialog.tsx`
  - `src/views/EncodingBox.tsx`
  - `src/views/ExplComponents.tsx`
  - `src/views/MessageSnackbar.tsx`
  - `src/views/ModelSelectionDialog.tsx`
  - `src/views/ReportView.tsx`
  - `src/views/TableSelectionView.tsx`
  - `src/views/VisualizationView.tsx`

- **diff 统计**（`git diff --stat`）：
  - `py-src/data_formulator/agent_routes.py`：+81 行（主要是 SSE/测试增强等）
  - `py-src/data_formulator/agents/client_utils.py`：+161 行（OpenAI/llama.cpp 兼容 fallback、流式兼容等）
  - 前端若干文件：共 +291 / -70（主要是中文化 `t('...')` 恢复/少量 UI 文案、以及构建修复）

## 关键改动说明（按模块）

### A. 中文化恢复（i18n）
- **新增/增强脚本**：`tools/restore_i18n_from_backup.py`
  - 从备份 ref 自动对比恢复 `t('...')` 调用
  - 处理多行 import 插入安全（避免插入到 `{}` 内）
  - 逐行替换并跳过模板字符串，减少 JSX 结构破坏风险
  - 仅替换 JSX 文本节点，避免在对象字面量等位置引入语法错误

- **前端恢复范围**：`src/views/*`、`src/app/App.tsx` 等多处文本恢复为 `t('...')`。

### B. llama.cpp（OpenAI compatible）联通修复
#### 1) `py-src/data_formulator/agent_routes.py`
- **移除对 `api_base` 的 HTML 转义**：避免 URL 被破坏导致请求异常。
- **增加（脱敏）诊断日志**：打印 endpoint/model/api_base/api_version，便于排查配置是否生效。
- **放宽 test-model 判定**：不再要求模型严格回复 `I can hear you.`，只要请求成功并返回 `choices` 即判定 ok（避免本地模型/推理输出导致误判）。

#### 2) `py-src/data_formulator/agents/client_utils.py`
- **OpenAI SDK -> httpx fallback**：当 OpenAI SDK 对本地 llama.cpp 抛 5xx（如 502）时，自动用 httpx 直连 `/v1/chat/completions`。
- **禁用系统代理影响**：httpx 使用 `trust_env=False`，避免 `HTTP_PROXY/HTTPS_PROXY` 等变量导致请求走代理产生 502。
- **SSE 流式兼容**：当 `stream=True` 时，fallback 返回可迭代 chunk（包含 `choices[0].delta.content`），兼容上层 `for part in stream:` 的用法。
- **失败时输出响应头/体片段**：便于定位到底是 llama.cpp 还是中间层返回错误。
- **多模态输入降级（临时策略）**：检测到 `messages[].content` 为 list 且包含 `image_url` 时，fallback 会剥离图片，仅保留 text，避免把图片发到纯文本模型导致 `image input is not supported`。
  - 注意：若你确实有多模态模型，应做“按模型能力路由”，而不是固定剥离（见下方建议）。

### C. “获取灵感/推荐问题”流式输出修复
- **`/api/agent/get-recommendation-questions` 改为标准 SSE**：
  - Response `mimetype` 设为 `text/event-stream`
  - 每条事件输出 `data: <payload>\n\n`
  - 增加 `Cache-Control: no-cache`、`X-Accel-Buffering: no` 防止缓冲导致前端只收到半条。

#### 进一步修复：偶发“无数据返回 / 前端无完整日志”
- `py-src/data_formulator/agents/agent_interactive_explore.py`
  - **为推荐问题流式调用显式传入 `max_tokens=1536`**：避免输出被默认上限截断（截断后会导致前端 JSON 解析失败，看起来像“没有返回数据”）。
- `src/views/ChartRecBox.tsx`
  - **SSE 按事件分隔符 `\n\n` 解析**：比 `split('data: ')` 更稳健，避免分块边界导致解析错位。
  - **增加解析失败日志**：`JSON.parse` 失败时输出截断片段，便于定位到底是哪一段 payload 被截断或格式不符合预期。

### D. “生成报告”模型选择与内容完整性修复
#### 1) `src/views/ReportView.tsx`
- **不再强依赖 `modelSlots.generation`**：
  - 当 generation 槽位不存在/为空时，自动回退使用 `dfSelectors.getActiveModel`（当前激活模型）。
  - 目的：避免升级后 `modelSlots` 机制缺失导致报错“尚未为生成槽位选择模型”。

#### 2) `py-src/data_formulator/agents/client_utils.py`
- **支持按场景传入 `max_tokens`**：`Client.get_completion(..., max_tokens=...)`。
- **本地 llama.cpp（httpx 直连）默认 stream 上限回到 512**：避免短任务（如推荐问题）无谓变慢。
- **当显式传入 `max_tokens` 时优先使用**：允许报告等长文本输出更完整。
- **非流式 fallback 兼容性修复**：补齐 `choice.message.role = "assistant"`，避免上层 agent 访问 `choice.message.role` 导致 500。

#### 3) `py-src/data_formulator/agents/agent_report_gen.py`
- **生成报告显式使用 `max_tokens=4096`**：减少 gpt-oss-20b 这类文本模型生成到一半被截断的问题。
- **恢复图表占位符输出规则**：提示词要求需要插图时输出占位符 `\`[IMAGE(chart_id)]\``（单独一行），以便前端把占位符替换成图表内容。

### E. “数据整理/代码解释”稳定性修复
#### 1) `py-src/data_formulator/agents/agent_py_data_rec.py`
- **数据整理（derive-data）显式使用 `max_tokens=2048`**：避免本地 llama.cpp 非流式默认上限过小导致回复被截断、缺失 ```python 代码块，从而报错 `No code block found in the response`。

#### 2) `py-src/data_formulator/agents/agent_code_explanation.py`
- **代码解释（code-expl）显式使用 `max_tokens=1536`**：降低回复被截断导致解释/概念 JSON 丢失的概率。
- **更健壮的解析策略**：
  - 若模型未输出 `[CODE EXPLANATION]`，则用全文作为解释文本；
  - 若模型未输出 `[CONCEPTS EXPLANATION]`，则尝试从全文提取 ```json 代码块；
  - 修复 `/api/agent/code-expl` 偶发 400（日志：`unable to extract JSON from response`）。

## 当前已验证通过的行为
- `yarn build` 通过（中文化脚本修复后的构建）。
- llama.cpp 端点可用（curl 调用 `/v1/models`、`/v1/chat/completions` 成功）。
- Data Formulator 后端不再因 502 失败（通过 httpx fallback + trust_env=False 规避）。

## 仍需确认/待优化项
- **多模态路由**：当前 fallback 在检测到 `image_url` 时会剥离图片以保证纯文本模型可用。
  - 如果你已部署多模态模型（例如 OCR/视觉模型在 8082），建议增加“有 image_url 则走多模态 endpoint”的路由策略（而不是剥离）。

## 升级后是否每次都要做这么多改动？（结论与建议）
- **不一定**。这次“看起来改动很多”的主要原因是：
  - i18n 被升级覆盖、且需要批量恢复到大量文件；
  - 本地 llama.cpp 适配涉及 SDK/代理/SSE/流式等一系列兼容点。

为了让下次升级成本极低，建议：
1. **把中文化恢复变成“可重复执行”流程**
   - 保留 `tools/restore_i18n_from_backup.py`，升级后直接跑一次（dry-run -> apply）。
2. **把后端 llama.cpp 兼容修复长期保留为补丁/分支**
   - 推荐做法：在你自己的分支（如 `local-custom`）持续维护这些后端改动；升级时对 upstream 做 rebase/merge。
3. **增加最小回归检查清单**（升级后 5 分钟内验证完）
   - `yarn build`
   - UI 模型测试（test-model 返回 ok）
   - “获取灵感”可返回多条问题（SSE 不截断）
4. **多模态明确路由**
   - 文本模型与多模态模型分端口/分 model id，后端按 `messages` 是否包含图片选择 endpoint。

## 如何导出完整补丁（可选）
如果你还希望把“今天全部未提交改动”保存成可直接 apply 的 patch：

```bash
git diff > patch-2025-12-24.diff
```

（该文件会包含所有未提交 diff，升级后可直接 `git apply`。）
